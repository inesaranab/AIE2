{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94e18d90",
   "metadata": {},
   "source": [
    "# Evaluating RAG with Ragas: NAIVE vs. SEMANTIC CHUNKING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e0d956c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI setup\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"Enter your OpenAI API key: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ebcb0c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangSmith setup for visualization\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass(\"Enter your LangSmith API key: \")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG-Evaluation-Ragas\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947cddd9",
   "metadata": {},
   "source": [
    "# Part 1: Synthetic Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5e73895b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "path = \"data/\"\n",
    "loader = DirectoryLoader(path, glob = \"*.pdf\", loader_cls=PyMuPDFLoader)\n",
    "docs = loader.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fd0766ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge Graph\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1\"))\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "76085ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d05f41ca62945569327dd03404148e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlinesExtractor:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a003ba14b2a74ae1a2a597935304c91e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying HeadlineSplitter:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n",
      "unable to apply transformation: 'headlines' property not found in this node\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c597ed481a3436cbac0b51d607411e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying SummaryExtractor:   0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Property 'summary' already exists in node '9e3dc2'. Skipping!\n",
      "Property 'summary' already exists in node '6e3b22'. Skipping!\n",
      "Property 'summary' already exists in node '76bf48'. Skipping!\n",
      "Property 'summary' already exists in node '38312a'. Skipping!\n",
      "Property 'summary' already exists in node '68f783'. Skipping!\n",
      "Property 'summary' already exists in node '3763a0'. Skipping!\n",
      "Property 'summary' already exists in node 'bd7f58'. Skipping!\n",
      "Property 'summary' already exists in node 'ee6eca'. Skipping!\n",
      "Property 'summary' already exists in node '44ad8d'. Skipping!\n",
      "Property 'summary' already exists in node 'd3a56e'. Skipping!\n",
      "Property 'summary' already exists in node '7325ae'. Skipping!\n",
      "Property 'summary' already exists in node 'ba319d'. Skipping!\n",
      "Property 'summary' already exists in node 'eb8a3c'. Skipping!\n",
      "Property 'summary' already exists in node 'd1d4c7'. Skipping!\n",
      "Property 'summary' already exists in node '1f0f77'. Skipping!\n",
      "Property 'summary' already exists in node 'e8af42'. Skipping!\n",
      "Property 'summary' already exists in node '0b91c7'. Skipping!\n",
      "Property 'summary' already exists in node 'c946f7'. Skipping!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270f294892af4e42b3eecfd42e40ec34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying CustomNodeFilter:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9c45ce11d14900880c6e4932415c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Property 'summary_embedding' already exists in node 'e8af42'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '76bf48'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'ee6eca'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '9e3dc2'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'd1d4c7'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'ba319d'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '6e3b22'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '3763a0'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '0b91c7'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '38312a'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'bd7f58'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'd3a56e'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '7325ae'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '68f783'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '44ad8d'. Skipping!\n",
      "Property 'summary_embedding' already exists in node '1f0f77'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'c946f7'. Skipping!\n",
      "Property 'summary_embedding' already exists in node 'eb8a3c'. Skipping!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6269dc4211ea41cfa272d9af0c5d3841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying [CosineSimilarityBuilder, OverlapScoreBuilder]:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6897b9b4a5d64330b47df05723563fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319d3768b8d6438a9038815f96bf75c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Scenarios:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea3908fe5874d74abe61fd66d591bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Samples:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "\n",
    "generator = TestsetGenerator(llm= generator_llm, embedding_model= generator_embeddings)\n",
    "dataset = generator.generate_with_langchain_docs(docs, testset_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0786d009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What does Acemoglu (2024) focus on regarding a...</td>\n",
       "      <td>[Introduction ChatGPT launched in November 202...</td>\n",
       "      <td>Acemoglu (2024) focuses on the effects of arti...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Could you provide a comprehensive explanation ...</td>\n",
       "      <td>[Introduction ChatGPT launched in November 202...</td>\n",
       "      <td>A Large Language Model (LLM) is a type of Arti...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what is like the most common Generalized Work ...</td>\n",
       "      <td>[Variation by Occupation Figure 23 presents va...</td>\n",
       "      <td>The context says that the most common Generali...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What occupation category does SOC2 code 19 rep...</td>\n",
       "      <td>[Variation by Occupation Figure 23 presents va...</td>\n",
       "      <td>SOC2 code 19 represents engineering and scienc...</td>\n",
       "      <td>single_hop_specifc_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How does the rapid adoption and diffusion of C...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nIntroduction ChatGPT launched in N...</td>\n",
       "      <td>The rapid adoption and diffusion of ChatGPT is...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How does ChatGPT usage for work-related purpos...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nConclusion This paper studies the ...</td>\n",
       "      <td>ChatGPT usage for work-related purposes in man...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>how chatgpt get adopted so fast and what diffe...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nConclusion This paper studies the ...</td>\n",
       "      <td>chatgpt got adopted real fast after launching ...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How does the rapid adoption and diffusion of C...</td>\n",
       "      <td>[&lt;1-hop&gt;\\n\\nConclusion This paper studies the ...</td>\n",
       "      <td>The rapid adoption and diffusion of ChatGPT an...</td>\n",
       "      <td>multi_hop_abstract_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What does Acemoglu (2024) focus on regarding a...   \n",
       "1  Could you provide a comprehensive explanation ...   \n",
       "2  what is like the most common Generalized Work ...   \n",
       "3  What occupation category does SOC2 code 19 rep...   \n",
       "4  How does the rapid adoption and diffusion of C...   \n",
       "5  How does ChatGPT usage for work-related purpos...   \n",
       "6  how chatgpt get adopted so fast and what diffe...   \n",
       "7  How does the rapid adoption and diffusion of C...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [Introduction ChatGPT launched in November 202...   \n",
       "1  [Introduction ChatGPT launched in November 202...   \n",
       "2  [Variation by Occupation Figure 23 presents va...   \n",
       "3  [Variation by Occupation Figure 23 presents va...   \n",
       "4  [<1-hop>\\n\\nIntroduction ChatGPT launched in N...   \n",
       "5  [<1-hop>\\n\\nConclusion This paper studies the ...   \n",
       "6  [<1-hop>\\n\\nConclusion This paper studies the ...   \n",
       "7  [<1-hop>\\n\\nConclusion This paper studies the ...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Acemoglu (2024) focuses on the effects of arti...   \n",
       "1  A Large Language Model (LLM) is a type of Arti...   \n",
       "2  The context says that the most common Generali...   \n",
       "3  SOC2 code 19 represents engineering and scienc...   \n",
       "4  The rapid adoption and diffusion of ChatGPT is...   \n",
       "5  ChatGPT usage for work-related purposes in man...   \n",
       "6  chatgpt got adopted real fast after launching ...   \n",
       "7  The rapid adoption and diffusion of ChatGPT an...   \n",
       "\n",
       "                       synthesizer_name  \n",
       "0  single_hop_specifc_query_synthesizer  \n",
       "1  single_hop_specifc_query_synthesizer  \n",
       "2  single_hop_specifc_query_synthesizer  \n",
       "3  single_hop_specifc_query_synthesizer  \n",
       "4  multi_hop_abstract_query_synthesizer  \n",
       "5  multi_hop_abstract_query_synthesizer  \n",
       "6  multi_hop_abstract_query_synthesizer  \n",
       "7  multi_hop_abstract_query_synthesizer  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3943d2",
   "metadata": {},
   "source": [
    "# 2. Baseline RAG PIPELINE - NAIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ac587acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275\n"
     ]
    }
   ],
   "source": [
    "# Langchain RAG pipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "# R - Retrieval\n",
    "\n",
    "# Load\n",
    "path = \"data/\"\n",
    "loader = DirectoryLoader(path, glob = \"*.pdf\", loader_cls=PyMuPDFLoader)\n",
    "docs = loader.load()\n",
    "\n",
    "# Chunk\n",
    "text_splitter= RecursiveCharacterTextSplitter(chunk_size= 500, chunk_overlap=0)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "print(len(split_documents))\n",
    "\n",
    "# Embed\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Qdrant Vector Store\n",
    "client = QdrantClient(\":memory:\")\n",
    "client.create_collection(\n",
    "    collection_name = 'advanced_build_data',\n",
    "    vectors_config = VectorParams(size=1536, distance= Distance.COSINE)\n",
    ")\n",
    "vector_store = QdrantVectorStore(\n",
    "    client = client,\n",
    "    collection_name = 'advanced_build_data',\n",
    "    embedding = embeddings\n",
    "\n",
    ")\n",
    "\n",
    "# Add documents\n",
    "_ = vector_store.add_documents(split_documents)\n",
    "\n",
    "# Retriever\n",
    "retriever = vector_store.as_retriever(search_kwargs = {'k': 3})\n",
    "\n",
    "# Node retieve\n",
    "def retrieve(state):\n",
    "    retrieved_docs = retriever.invoke(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "# A - Augmented\n",
    "\n",
    "RAG_PROMPT = \"\"\"\\\n",
    "You are a helpful assistant who answers questions based on provided context. You must only use the provided context, and cannot use your own knowledge.\n",
    "\n",
    "### Question\n",
    "{question}\n",
    "\n",
    "### Context\n",
    "{context}\n",
    "\"\"\"\n",
    "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT)\n",
    "\n",
    "# G - Generation\n",
    "\n",
    "# model for generation \n",
    "llm = ChatOpenAI(model = \"gpt-4.1-nano\")\n",
    "\n",
    "# node\n",
    "def generate(state):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = rag_prompt.format_messages(question=state[\"question\"], context=docs_content)\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"response\" : response.content}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c520eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# State\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    response: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bcba35fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "naive_graph= graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf1d58a",
   "metadata": {},
   "source": [
    "# 3. Improved RAG PIPELINE - SEMANTIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a6afde",
   "metadata": {},
   "source": [
    "We are going to  use semantic chunking strategy:\n",
    " \n",
    "https://python.langchain.com/api_reference/experimental/text_splitter/langchain_experimental.text_splitter.SemanticChunker.html\n",
    "\n",
    "Specifically we are going to implement semantic chunking strategy that:\n",
    "- Groups semantically similar sentences together\n",
    "- Uses a threshold to determine similarity\n",
    "- Chunks paragraphs greedily up to max size\n",
    "- Minimum chunk = single sentence\n",
    "\n",
    "SemanticChunker grows chunks while sentences stay semantically close; it cuts only at semantic “breakpoints.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "574c06ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Semantic chunking strategy\n",
    "\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "semantic_splitter = SemanticChunker(\n",
    "    OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
    "    breakpoint_threshold_type=\"percentile\",\n",
    "    breakpoint_threshold_amount=77,      # 90–99: lower -> more cuts; higher -> fewer cuts\n",
    "    sentence_split_regex=r\"(?<=[.?!])\\s+\", #regex for sentence split\n",
    "    min_chunk_size=None\n",
    ")\n",
    "\n",
    "split_documents = semantic_splitter.split_documents(docs)\n",
    "len(split_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "69e6a7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"advanced_build_data\",\n",
    "    vectors_config=VectorParams(size=1536, distance=Distance.COSINE),\n",
    ")\n",
    "\n",
    "vector_store = QdrantVectorStore(\n",
    "    client = client,\n",
    "    collection_name = \"advanced_build_data\",\n",
    "    embedding = embeddings\n",
    ")\n",
    "\n",
    "_ = vector_store.add_documents(documents=split_documents)\n",
    "\n",
    "semantic_retriever = vector_store.as_retriever(search_kwargs = {'k': 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2d16ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted node for retrieval\n",
    "def retrieved_semantic(state):\n",
    "    retrieved_docs = semantic_retriever.invoke(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "df728e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild the graph with the new retriever\n",
    "\n",
    "class SemanticState(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    response: str\n",
    "\n",
    "semantic_graph_builder = StateGraph(SemanticState).add_sequence([retrieved_semantic, generate])\n",
    "semantic_graph_builder.add_edge(START, \"retrieved_semantic\")\n",
    "semantic_graph = semantic_graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68862a94",
   "metadata": {},
   "source": [
    "# 4. RAGAS Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4b50e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# judge model\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model = \"gpt-4.1-mini\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e9cc9b",
   "metadata": {},
   "source": [
    "#### The metrics:\n",
    "\n",
    "#### Faithfulness \n",
    "\n",
    "The Faithfulness metric measures how factually consistent a **response** is with the **retrieved context**. It ranges from 0 to 1, with higher scores indicating better consistency.\n",
    "\n",
    "A response is considered faithful if all its claims can be supported by the retrieved context.\n",
    "\n",
    "\n",
    "#### Response Relevancy\n",
    "\n",
    "Response Relevancy, focuses on assessing how pertinent the generated answer is to the given prompt. A lower score is assigned to answers that are incomplete or contain redundant information and higher scores indicate better relevancy. \n",
    "\n",
    "**The metric turns your answer into one or more hypothetical questions and then compares those (via embeddings) to the original question. If the answer is on-topic, the questions it implies will be close to the user’s question.**\n",
    "\n",
    "This metric is computed using the **user_input**, the **retrieved_context** and the **response**.\n",
    "\n",
    "The Answer Relevancy is defined as the mean cosine similarity of the original question to a number of artifical questions, which where generated (reverse engineered) based on the answer.\n",
    "\n",
    "*Why the context?* It is also shown to generate the answer.\n",
    "\n",
    "#### Context Precision\n",
    "\n",
    "Context Precision is a metric that evaluates the retriever’s ability to rank relevant chunks higher than irrelevant ones for a given query in the retrieved context. Specifically, it assesses the **degree to which relevant chunks in the retrieved context are placed at the top of the ranking**.\n",
    "\n",
    "It is calculated as the mean of the precision@k for each chunk in the context. Precision@k is the ratio of the number of relevant chunks at rank k to the total number of chunks at rank k.\n",
    "\n",
    "In RAGAS, it uses *LLMContextPrecisionWithReference*, that means, to estimate if the retrieved contexts are relevant, this method uses the LLM to compare each chunk in **retrieved_contexts** with the **reference**. \n",
    "\n",
    "You will be needing: **user_input**, **retrived_contexts**, **reference**.\n",
    "\n",
    "#### Context Recall\n",
    "\n",
    "Context Recall measures how many of the relevant documents (or pieces of information) were successfully retrieved. It focuses on not missing important results. Higher recall means fewer relevant documents were left out. In short, recall is about not missing anything important. Since it is about not missing anything, calculating context recall always requires a reference to compare against.\n",
    "\n",
    "In new RAGAS we can use an LLM based approach: **LLMContextRecall**. It is computed using **user_input**, **reference** and the **retrieved_contexts**, and the values range between 0 and 1, with higher values indicating better performance. \n",
    "This metric uses reference as a proxy to reference_contexts which also makes it easier to use as annotating reference contexts can be very time-consuming. **To estimate context recall from the reference, the reference is broken down into claims each claim in the reference answer is analyzed to determine whether it can be attributed to the retrieved context or not**. In an ideal scenario, all claims in the reference answer should be attributable to the retrieved context.\n",
    "\n",
    "#### Answer Correctness\n",
    "\n",
    "The assessment of Answer Correctness involves gauging the accuracy of the **generated answer** when compared to the **ground truth**. This evaluation relies on the ground truth and the answer, with scores ranging from 0 to 1. A higher score indicates a closer alignment between the generated answer and the ground truth, signifying better correctness.\n",
    "\n",
    "Answer correctness encompasses two critical aspects: semantic similarity between the generated answer and the ground truth, as well as factual similarity. These aspects are combined using a weighted scheme to formulate the answer correctness score. Users also have the option to employ a ‘threshold’ value to round the resulting score to binary, if desired.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e984aee",
   "metadata": {},
   "source": [
    "#### Baseline system eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6aa9c22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the synthetic questions through the baseline RAG pipeline\n",
    "for test_row in dataset:\n",
    "  response = naive_graph.invoke({\"question\" : test_row.eval_sample.user_input})\n",
    "  test_row.eval_sample.response = response[\"response\"]\n",
    "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "12bed6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert synthetic dataset to evaluation format\n",
    "from ragas import EvaluationDataset\n",
    "\n",
    "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "62f3db77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243be7309c1948a1a504259136747584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate Baseline RAG system\n",
    "\n",
    "from ragas.metrics import Faithfulness, ResponseRelevancy, LLMContextPrecisionWithReference, LLMContextRecall, AnswerCorrectness\n",
    "from ragas import evaluate, RunConfig\n",
    "\n",
    "custom_run_config = RunConfig(timeout=360)\n",
    "\n",
    "\n",
    "baseline_result = evaluate(\n",
    "    dataset = evaluation_dataset,\n",
    "    metrics = [Faithfulness(), ResponseRelevancy(), LLMContextPrecisionWithReference(), LLMContextRecall(), AnswerCorrectness()],\n",
    "    llm = evaluator_llm,\n",
    "    run_config = custom_run_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f1870aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 0.6912, 'answer_relevancy': 0.8189, 'llm_context_precision_with_reference': 0.8958, 'context_recall': 0.5326, 'answer_correctness': 0.5765}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the results\n",
    "baseline_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1404ed2c",
   "metadata": {},
   "source": [
    "#### Semantic system eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "2b574646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the synthetic questions through the semantic RAG pipeline\n",
    "import copy\n",
    "\n",
    "semantic_dataset = copy.deepcopy(dataset)\n",
    "\n",
    "for test_row in semantic_dataset:\n",
    "  response = semantic_graph.invoke({\"question\" : test_row.eval_sample.user_input})\n",
    "  test_row.eval_sample.response = response[\"response\"]\n",
    "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5b835e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert synthetic dataset to evaluation format\n",
    "\n",
    "semantic_evaluation_dataset = EvaluationDataset.from_pandas(semantic_dataset.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3bb98f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd188a21fbd4b6ba10a46faaeec70e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate semantic RAG system\n",
    "semantic_result = evaluate(\n",
    "    dataset=semantic_evaluation_dataset,\n",
    "    metrics=[Faithfulness(), ResponseRelevancy(), LLMContextPrecisionWithReference(), LLMContextRecall(), AnswerCorrectness() ],\n",
    "    llm=evaluator_llm,\n",
    "    run_config=custom_run_config\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6008ceea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 0.7997, 'answer_relevancy': 0.9328, 'llm_context_precision_with_reference': 1.0000, 'context_recall': 0.6362, 'answer_correctness': 0.5437}"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semantic_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ebfef7",
   "metadata": {},
   "source": [
    "# 5: Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "37f3f776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 numeric metric columns: ['faithfulness', 'answer_relevancy', 'llm_context_precision_with_reference', 'context_recall', 'answer_correctness']\n",
      "\n",
      "=== METRIC COMPARISON ===\n",
      "                              Metric  Baseline  Semantic   Delta\n",
      "                        faithfulness    0.6912    0.7997  0.1085\n",
      "                    answer_relevancy    0.8189    0.9328  0.1139\n",
      "llm_context_precision_with_reference    0.8958    1.0000  0.1042\n",
      "                      context_recall    0.5326    0.6362  0.1036\n",
      "                  answer_correctness    0.5765    0.5437 -0.0328\n"
     ]
    }
   ],
   "source": [
    "# Analysis: Baseline vs Semantic Chunking Results\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Extract metrics from both results\n",
    "baseline_metrics = baseline_result.to_pandas()\n",
    "semantic_metrics = semantic_result.to_pandas()\n",
    "\n",
    "# Find only numeric metric columns\n",
    "metric_columns = []\n",
    "for col in baseline_metrics.columns:\n",
    "    if col not in ['question', 'answer', 'contexts', 'ground_truth', 'user_input', 'response', 'retrieved_contexts', 'reference_contexts']:\n",
    "        # Check if column contains numeric data\n",
    "        try:\n",
    "            pd.to_numeric(baseline_metrics[col], errors='raise')\n",
    "            metric_columns.append(col)\n",
    "        except (ValueError, TypeError):\n",
    "            continue\n",
    "\n",
    "print(f\"Found {len(metric_columns)} numeric metric columns: {metric_columns}\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_data = {\n",
    "    'Metric': metric_columns,\n",
    "    'Baseline': [round(baseline_metrics[col].mean(), 4) for col in metric_columns],\n",
    "    'Semantic': [round(semantic_metrics[col].mean(), 4) for col in metric_columns]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df['Delta'] = comparison_df['Semantic'] - comparison_df['Baseline']\n",
    "\n",
    "print(\"\\n=== METRIC COMPARISON ===\")\n",
    "print(comparison_df.to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
